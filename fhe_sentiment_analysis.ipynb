{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tenseal"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kz4Bye8YTVgG",
        "outputId": "091166cb-683a-459f-d640-bde3a26ee8e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tenseal in /usr/local/lib/python3.10/dist-packages (0.3.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0I7b0p9PLC1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import re\n",
        "\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "import tenseal as ts"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "base_dir = '/content/drive/MyDrive/fhe-embedding/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCsyH9qbPmOp",
        "outputId": "9be6b0ac-f6aa-4cae-ac90-33759015a67f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### References\n",
        "- https://github.com/OpenMined/TenSEAL\n",
        "- https://arxiv.org/abs/2104.03152\n",
        "- https://arxiv.org/pdf/2202.00004.pdf"
      ],
      "metadata": {
        "id": "UkS111FRmiwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Larson\n",
        "def load_embeddings(filename):\n",
        "    \"\"\"\n",
        "    Load a DataFrame from the generalized text format used by word2vec, GloVe,\n",
        "    fastText, and ConceptNet Numberbatch. The main point where they differ is\n",
        "    whether there is an initial line with the dimensions of the matrix.\n",
        "    \"\"\"\n",
        "    labels = []\n",
        "    rows = []\n",
        "    with open(filename, encoding='utf-8') as infile:\n",
        "        for i, line in enumerate(infile):\n",
        "            items = line.rstrip().split(' ')\n",
        "            if len(items) == 2:\n",
        "                # This is a header row giving the shape of the matrix\n",
        "                continue\n",
        "            labels.append(items[0])\n",
        "            values = np.array([float(x) for x in items[1:]], 'f')\n",
        "            rows.append(values)\n",
        "\n",
        "    arr = np.vstack(rows)\n",
        "    return pd.DataFrame(arr, index=labels, dtype='f')\n",
        "\n",
        "embeddings = load_embeddings(base_dir + 'glove.6B.300d.txt')\n",
        "embeddings.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxV0CpWOP06F",
        "outputId": "8573da73-6618-4dd6-af92-9f27a7e27c45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 41.5 s, sys: 2.55 s, total: 44 s\n",
            "Wall time: 1min\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(400001, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Larson\n",
        "def load_lexicon(filename):\n",
        "    \"\"\"\n",
        "    Load a file from Bing Liu's sentiment lexicon\n",
        "    (https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html), containing\n",
        "    English words in Latin-1 encoding.\n",
        "\n",
        "    One file contains a list of positive words, and the other contains\n",
        "    a list of negative words. The files contain comment lines starting\n",
        "    with ';' and blank lines, which should be skipped.\n",
        "    \"\"\"\n",
        "    lexicon = []\n",
        "    with open(filename, encoding='latin-1') as infile:\n",
        "        for line in infile:\n",
        "            line = line.rstrip()\n",
        "            if line and not line.startswith(';'):\n",
        "                lexicon.append(line)\n",
        "    return lexicon\n",
        "\n",
        "pos_words = load_lexicon(base_dir + 'positive-words.txt')\n",
        "neg_words = load_lexicon(base_dir + 'negative-words.txt')\n",
        "\n",
        "print(len(pos_words), len(neg_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gn9MIdrrdWzY",
        "outputId": "749c024a-2ca4-4327-fd8e-a0981d0ced7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2006 4783\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Larson\n",
        "def vecs_to_sentiment(vecs):\n",
        "    # predict_log_proba gives the log probability for each class\n",
        "    # predictions = model.predict_log_proba(vecs)\n",
        "    predictions = model.predict_log_proba(vecs).numpy().ravel()\n",
        "\n",
        "    # To see an overall positive vs. negative classification in one number,\n",
        "    # we take the log probability of positive sentiment minus the log\n",
        "    # probability of negative sentiment.\n",
        "    # this is a logarithm of the max margin for the classifier,\n",
        "    # similar to odds ratio (but not exact) log(p_1/p_0) = log(p_1)-log(p_0)\n",
        "    # return predictions[:, 1] - predictions[:, 0]\n",
        "    return predictions\n",
        "\n",
        "def enc_vecs_to_sentiment(vecs):\n",
        "    \"\"\"\n",
        "    simulating a client-server interaction\n",
        "      client: data owner that encrypts the data and later decrypts the results\n",
        "      server: contains the machine learning model (logistic regression in this case) and receives encrypted data and returns encrypted results thereby preserving complete data privacy\n",
        "\n",
        "    purpose: the client does not need to host a potentially large and memory intensive model nor does the client have to spend computational resources evaluating the model.\n",
        "      Instead, client simply encrypts the desired data at the start and decrypts the results at the end.\n",
        "      The server, presumably a third party with practically unlimited computational resources, hosts the model and handles machine learning computations with no access to the data in plaintext\n",
        "    \"\"\"\n",
        "\n",
        "    # client-side: encrypt data\n",
        "    vecs = torch.Tensor(vecs.values)\n",
        "    enc_vecs = [ts.ckks_vector(ctx_eval, x.tolist()) for x in vecs]\n",
        "\n",
        "    # server-side: compute evaluation on encrypted data\n",
        "    forwards = []\n",
        "    for enc_vec in enc_vecs:\n",
        "      forward = enc_lr.predict_log_proba(enc_vec)\n",
        "      forwards.append(forward)\n",
        "\n",
        "    # client-side: decrypt the results and return results in plaintext\n",
        "    forwards = [forward.decrypt() for forward in forwards]\n",
        "\n",
        "    forwards = torch.Tensor(forwards)\n",
        "    out = torch.sigmoid(forwards)\n",
        "    with torch.no_grad():\n",
        "      predictions = torch.log(out) - torch.log(1 - out)\n",
        "\n",
        "    predictions = predictions.numpy().ravel()\n",
        "\n",
        "    return predictions\n",
        "\n",
        "def words_to_sentiment(words):\n",
        "    vecs = embeddings.loc[words].dropna()\n",
        "    log_odds = vecs_to_sentiment(vecs)\n",
        "    return pd.DataFrame({'sentiment': log_odds}, index=vecs.index)\n",
        "\n",
        "def enc_words_to_sentiment(words):\n",
        "    vecs = embeddings.loc[words].dropna()\n",
        "    log_odds = enc_vecs_to_sentiment(vecs)\n",
        "    return pd.DataFrame({'sentiment': log_odds}, index=vecs.index)"
      ],
      "metadata": {
        "id": "T9udy2tydrLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Larson\n",
        "import re\n",
        "TOKEN_RE = re.compile(r\"\\w.*?\\b\")\n",
        "# The regex above finds tokens that start with a word-like character (\\w), and continues\n",
        "# matching characters (.+?) until the next word break (\\b). It's a relatively simple\n",
        "# expression that manages to extract something very much like words from text.\n",
        "\n",
        "def text_to_sentiment(text):\n",
        "    # tokenize the input phrase\n",
        "    tokens = [token.casefold() for token in TOKEN_RE.findall(text)]\n",
        "    # send each token separately into the embedding, then the classifier\n",
        "    sentiments = words_to_sentiment(tokens)\n",
        "    return sentiments['sentiment'].mean() # return the mean for the classifier\n",
        "\n",
        "def enc_text_to_sentiment(text):\n",
        "    # tokenize the input phrase\n",
        "    tokens = [token.casefold() for token in TOKEN_RE.findall(text)]\n",
        "    # send each token separately into the embedding, then the classifier\n",
        "    sentiments = enc_words_to_sentiment(tokens)\n",
        "    return sentiments['sentiment'].mean() # return the mean for the classifier"
      ],
      "metadata": {
        "id": "dwskcbzpdtGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "jj8W27FmgI-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Larson\n",
        "pos_words_common = list(set(pos_words) & set(embeddings.index))\n",
        "neg_words_common = list(set(neg_words) & set(embeddings.index))\n",
        "\n",
        "pos_vectors = embeddings.loc[pos_words_common]\n",
        "neg_vectors = embeddings.loc[neg_words_common]\n",
        "print(pos_vectors.shape,neg_vectors.shape)\n",
        "\n",
        "vectors = pd.concat([pos_vectors, neg_vectors])\n",
        "targets = np.array([1 for entry in pos_vectors.index] + [0 for entry in neg_vectors.index])\n",
        "labels = list(pos_vectors.index) + list(neg_vectors.index)\n",
        "\n",
        "vectors = (vectors - vectors.mean()) / vectors.std()\n",
        "vectors = torch.Tensor(vectors.values)\n",
        "targets = torch.Tensor(targets.reshape(-1,1))\n",
        "\n",
        "train_vectors, test_vectors, train_targets, test_targets, train_labels, test_labels = \\\n",
        "    train_test_split(vectors, targets, labels, test_size=0.2, random_state=0)\n",
        "\n",
        "# train_vectors = torch.Tensor(train_vectors.values)\n",
        "# train_targets = torch.Tensor(train_targets.reshape(-1,1))\n",
        "# test_vectors = torch.Tensor(test_vectors.values)\n",
        "# test_targets = torch.Tensor(test_targets.reshape(-1,1))\n",
        "\n",
        "print(train_vectors.shape, train_targets.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7MgmtrPgMit",
        "outputId": "b9cedb02-8a40-44a5-bcff-ee87679caae1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1893, 300) (4345, 300)\n",
            "torch.Size([4990, 300]) torch.Size([4990, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### plaintext model"
      ],
      "metadata": {
        "id": "N_6kEA45icqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Larson\n",
        "\n",
        "# create a linear classifier\n",
        "model = SGDClassifier(loss='log_loss', random_state=0, max_iter=100)\n",
        "model.fit(train_vectors, train_targets)\n",
        "accuracy_score(model.predict(test_vectors), test_targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qcqKaUhhtaN",
        "outputId": "2aca0d38-8608-44ac-c980-be97c8f703d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8878205128205128"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### FHE model\n",
        "TenSEAL library documentation and source for code examples: https://github.com/OpenMined/TenSEAL"
      ],
      "metadata": {
        "id": "d2m8snpairGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LR(torch.nn.Module):\n",
        "  def __init__(self, n_features):\n",
        "    super(LR, self).__init__()\n",
        "    self.lr = torch.nn.Linear(n_features, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.sigmoid(self.lr(x))\n",
        "    # print(\"shape after sigmoid: \", out.shape)\n",
        "    return out\n",
        "\n",
        "  def predict_log_proba(self, x):\n",
        "    # print(x.shape)\n",
        "    x = torch.Tensor(x.values)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      pred = torch.log(self.forward(x)) - torch.log(1 - self.forward(x))\n",
        "    return pred"
      ],
      "metadata": {
        "id": "9hpKovarFt7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_features = train_vectors.shape[1]\n",
        "model = LR(n_features)\n",
        "optim = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "# use Binary Cross Entropy Loss\n",
        "criterion = torch.nn.BCELoss()"
      ],
      "metadata": {
        "id": "74XL8hdwTlHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 4500\n",
        "\n",
        "def train(model, optim, criterion, x, y, epochs=EPOCHS):\n",
        "  for e in range(1, epochs + 1):\n",
        "    optim.zero_grad()\n",
        "    out = model(x)\n",
        "    loss = criterion(out, y)\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    if e % 500 == 0:\n",
        "      print(f\"Loss at epoch {e}: {loss.data}\")\n",
        "  return model\n",
        "\n",
        "model = train(model, optim, criterion, train_vectors, train_targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GnfcaIXUD6x",
        "outputId": "c6c6fc1b-ae09-422f-aaeb-d51b0c44804d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss at epoch 500: 0.49766862392425537\n",
            "Loss at epoch 1000: 0.4027809500694275\n",
            "Loss at epoch 1500: 0.354069322347641\n",
            "Loss at epoch 2000: 0.32376620173454285\n",
            "Loss at epoch 2500: 0.30275776982307434\n",
            "Loss at epoch 3000: 0.28716495633125305\n",
            "Loss at epoch 3500: 0.27504006028175354\n",
            "Loss at epoch 4000: 0.2652883529663086\n",
            "Loss at epoch 4500: 0.25724294781684875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(model, x, y):\n",
        "    out = model(x)\n",
        "    correct = torch.abs(y - out) < 0.5\n",
        "    return correct.float().mean()\n",
        "\n",
        "plain_accuracy = accuracy(model, test_vectors, test_targets)\n",
        "print(f\"Accuracy on plain test_set: {plain_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IW50UGH9cfAk",
        "outputId": "0374587b-bd40-46a2-9b09-471ba9954b21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on plain test_set: 0.9102563858032227\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EncryptedLR:\n",
        "\n",
        "  def __init__(self, torch_lr):\n",
        "    # TenSEAL processes lists and not torch tensors,\n",
        "    # so we take out the parameters from the PyTorch model\n",
        "    self.weight = torch_lr.lr.weight.data.tolist()[0]\n",
        "    self.bias = torch_lr.lr.bias.data.tolist()\n",
        "\n",
        "  def forward(self, enc_x):\n",
        "    # We don't need to perform sigmoid as this model\n",
        "    # will only be used for evaluation, and the label\n",
        "    # can be deduced without applying sigmoid\n",
        "    enc_out = enc_x.dot(self.weight) + self.bias\n",
        "    return enc_out\n",
        "\n",
        "  def forward_approx(self, enc_x):\n",
        "    enc_out = enc_x.dot(self.weight) + self.bias\n",
        "    enc_out = sigmoid_approx(enc_out)\n",
        "    return enc_out\n",
        "\n",
        "  def predict_log_proba(self, enc_x):\n",
        "    enc_out = self.forward(enc_x)\n",
        "    return enc_out\n",
        "\n",
        "\n",
        "  def __call__(self, *args, **kwargs):\n",
        "    return self.forward(*args, **kwargs)\n",
        "\n",
        "  def encrypt(self, context):\n",
        "    self.weight = ts.ckks_vector(context, self.weight)\n",
        "    self.bias = ts.ckks_vector(context, self.bias)\n",
        "\n",
        "  def decrypt(self, context):\n",
        "    self.weight = self.weight.decrypt()\n",
        "    self.bias = self.bias.decrypt()\n",
        "\n",
        "\n",
        "enc_lr = EncryptedLR(model)"
      ],
      "metadata": {
        "id": "TJJvQ37a7Hol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parameters\n",
        "# poly_mod_degree = 4096\n",
        "# coeff_mod_bit_sizes = [40, 20, 40]\n",
        "poly_mod_degree = 8192\n",
        "coeff_mod_bit_sizes = [40, 21, 21, 21, 21, 21, 21, 40]\n",
        "# create TenSEALContext\n",
        "ctx_eval = ts.context(ts.SCHEME_TYPE.CKKS, poly_mod_degree, -1, coeff_mod_bit_sizes)\n",
        "# scale of ciphertext to use\n",
        "ctx_eval.global_scale = 2 ** 20\n",
        "# this key is needed for doing dot-product operations\n",
        "ctx_eval.generate_galois_keys()"
      ],
      "metadata": {
        "id": "ckVsYBl57cEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "enc_x_test = [ts.ckks_vector(ctx_eval, x.tolist()) for x in test_vectors]\n",
        "test_vectors.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayHX-_Yi7dYD",
        "outputId": "f0c1971b-36cd-41df-daf7-b75e73c1fc1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 17 s, sys: 779 ms, total: 17.8 s\n",
            "Wall time: 19.6 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1248, 300])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encrypted_evaluation(model, enc_x_test, y_test):\n",
        "\n",
        "  correct = 0\n",
        "  for enc_x, y in zip(enc_x_test, y_test):\n",
        "    # encrypted evaluation\n",
        "    enc_out = model(enc_x)\n",
        "    out = enc_out.decrypt()\n",
        "    out = torch.tensor(out)\n",
        "    out = torch.sigmoid(out)\n",
        "    if torch.abs(out - y) < 0.5:\n",
        "      correct += 1\n",
        "\n",
        "  print(f\"Evaluated test_set of {len(test_vectors)} entries\")\n",
        "  print(f\"Accuracy: {correct}/{len(test_vectors)} = {correct / len(test_vectors)}\")\n",
        "  return correct / len(test_vectors)\n",
        "\n",
        "encrypted_accuracy = encrypted_evaluation(enc_lr, enc_x_test, test_targets)\n",
        "diff_accuracy = plain_accuracy - encrypted_accuracy\n",
        "print(f\"Difference between plain and encrypted accuracies: {diff_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhBuXIZX7z_C",
        "outputId": "1a884d76-c10b-44e2-9cd0-a0b37f290212"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluated test_set of 1248 entries\n",
            "Accuracy: 899/1248 = 0.7203525641025641\n",
            "Difference between plain and encrypted accuracies: 0.18990379571914673\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encrypted_evaluation(model, enc_x_test, y_test):\n",
        "\n",
        "  correct = 0\n",
        "  for enc_x, y in zip(enc_x_test, y_test):\n",
        "    # encrypted evaluation\n",
        "    enc_out = model(enc_x)\n",
        "    out = sigmoid_approx(enc_out)\n",
        "    out = out.decrypt()\n",
        "    out = torch.tensor(out)\n",
        "    if torch.abs(out - y) < 0.5:\n",
        "      correct += 1\n",
        "\n",
        "  print(f\"Evaluated test_set of {len(test_vectors)} entries with sigmoid approximation\")\n",
        "  print(f\"Accuracy: {correct}/{len(test_vectors)} = {correct / len(test_vectors)}\")\n",
        "  return correct / len(test_vectors)\n",
        "\n",
        "encrypted_accuracy = encrypted_evaluation(enc_lr, enc_x_test, test_targets)\n",
        "diff_accuracy = plain_accuracy - encrypted_accuracy\n",
        "print(f\"Difference between plain and encrypted accuracies: {diff_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdgzyqQUSRuM",
        "outputId": "2da0a5fa-8720-492a-d53f-a1c9d6d9e037"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluated test_set of 1248 entries with sigmoid approximation\n",
            "Accuracy: 890/1248 = 0.7131410256410257\n",
            "Difference between plain and encrypted accuracies: 0.19711536169052124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid_approx(enc_x):\n",
        "    return enc_x.polyval([0.5, 0.197, 0, -0.004])"
      ],
      "metadata": {
        "id": "Ie_feFMF05Fv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "Qlm20RC2h5U4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Logistic Regression -- plaintext\n",
        "print(text_to_sentiment(\"This movie felt like a beautifully animated amusement park ride\"))\n",
        "print(text_to_sentiment(\"Not high art or anything, but it ticks off almost everything for what Mario should be at least\"))\n",
        "print(text_to_sentiment(\"easily one of the worst movie Illumination produced so far\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPhxskYPh4OG",
        "outputId": "0047354e-e8b0-4669-f28b-9d86bf02a35c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.2688787\n",
            "-0.10797867\n",
            "-0.08086337\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Logistic Regression -- FHE evaluation\n",
        "print(enc_text_to_sentiment(\"This movie felt like a beautifully animated amusement park ride\"))\n",
        "print(enc_text_to_sentiment(\"Not high art or anything, but it ticks off almost everything for what Mario should be at least\"))\n",
        "print(enc_text_to_sentiment(\"easily one of the worst movie Illumination produced so far\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJHgZyzh1I2C",
        "outputId": "215a0c42-83cc-4ba1-bb37-7f471967efab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-2.87028\n",
            "-3.0460367\n",
            "-2.989269\n"
          ]
        }
      ]
    }
  ]
}